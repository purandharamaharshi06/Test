{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"AxQMA_qCjo8G"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r0vfuWe1j81C"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout \n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.optimizers import Adam\n","\n","from sklearn.metrics import classification_report,confusion_matrix\n","\n","import tensorflow as tf\n","\n","import cv2\n","import os\n","\n","import numpy as np\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_oTYym_j-7_"},"outputs":[],"source":["labels = ['rugby', 'soccer']\n","img_size = 224\n","def get_data(data_dir):\n","    data = [] \n","    for label in labels: \n","        path = os.path.join(data_dir, label)\n","        class_num = labels.index(label)\n","        for img in os.listdir(path):\n","            try:\n","                img_arr = cv2.imread(os.path.join(path, img))[...,::-1] #convert BGR to RGB formats\n","                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping the images to preferred size\n","                data.append([resized_arr, class_num])\n","            except Exception as e:\n","                print(e)\n","    return np.array(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MDpavmDykKhB"},"outputs":[],"source":["Now we can easily fetch our train and validation data.\n","train = get_data('../input/traintestsports/Main/train')\n","val = get_data('../input/traintestsports/Main/test')\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"54LAQk64kLQc"},"outputs":[],"source":["x_train = []\n","y_train = []\n","x_val = []\n","y_val = []\n","\n","for feature, label in train:\n","  x_train.append(feature)\n","  y_train.append(label)\n","\n","for feature, label in val:\n","  x_val.append(feature)\n","  y_val.append(label)\n","\n","# Normalize the data\n","x_train = np.array(x_train) / 255\n","x_val = np.array(x_val) / 255\n","\n","x_train.reshape(-1, img_size, img_size, 1)\n","y_train = np.array(y_train)\n","\n","x_val.reshape(-1, img_size, img_size, 1)\n","y_val = np.array(y_val)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7G6AYuLzkUPG"},"outputs":[],"source":["datagen = ImageDataGenerator(\n","        featurewise_center=False,  # set input mean to 0 over the dataset\n","        samplewise_center=False,  # set each sample mean to 0\n","        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n","        samplewise_std_normalization=False,  # divide each input by its std\n","        zca_whitening=False,  # apply ZCA whitening\n","        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n","        zoom_range = 0.2, # Randomly zoom image \n","        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n","        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n","        horizontal_flip = True,  # randomly flip images\n","        vertical_flip=False)  # randomly flip images\n","\n","\n","datagen.fit(x_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Cyg1LxQkYqZ"},"outputs":[],"source":["model = Sequential()\n","model.add(Conv2D(32,3,padding=\"same\", activation=\"relu\", input_shape=(224,224,3)))\n","model.add(MaxPool2D())\n","\n","model.add(Conv2D(32, 3, padding=\"same\", activation=\"relu\"))\n","model.add(MaxPool2D())\n","\n","model.add(Conv2D(64, 3, padding=\"same\", activation=\"relu\"))\n","model.add(MaxPool2D())\n","model.add(Dropout(0.4))\n","\n","model.add(Flatten())\n","model.add(Dense(128,activation=\"relu\"))\n","model.add(Dense(2, activation=\"softmax\"))\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8NIBzYwikakj"},"outputs":[],"source":["opt = Adam(lr=0.000001)\n","model.compile(optimizer = opt , loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) , metrics = ['accuracy'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRk8AXkQkeef"},"outputs":[],"source":["history = model.fit(x_train,y_train,epochs = 500 , validation_data = (x_val, y_val))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"04-LTzNWkjHQ"},"outputs":[],"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(500)\n","\n","plt.figure(figsize=(15, 15))\n","plt.subplot(2, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(2, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iUU4A4vSkjsW"},"outputs":[],"source":["predictions = model.predict_classes(x_val)\n","predictions = predictions.reshape(1,-1)[0]\n","print(classification_report(y_val, predictions, target_names = ['Rugby (Class 0)','Soccer (Class 1)']))\n"]},{"cell_type":"markdown","metadata":{"id":"oVqtwreQYd7E"},"source":["Task 1: Run the above code with given dataset.\n","\n","Task 2: Run the code with different dataset\n","\n","\n","\n","\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO9OXlAt5WjE/YQp6pPFuYR","collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
